{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DTC Agent TPU Training",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# DTC Agent - TPU Training in Google Colab\n",
        "\n",
        "This notebook runs the Dual-Timescale Competence (DTC) reinforcement learning agent on Google Cloud TPU.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**IMPORTANT:** Make sure you have selected TPU as your runtime:\n",
        "1. Go to `Runtime` -> `Change runtime type`\n",
        "2. Select `TPU` as the hardware accelerator\n",
        "3. Click `Save`\n",
        "\n",
        "## What is DTC?\n",
        "\n",
        "DTC is a model-based RL agent that learns through intrinsic motivation:\n",
        "- **Learning Progress** (competence/mastery tracking)\n",
        "- **Clean Curiosity** (epistemic uncertainty)\n",
        "- **Empowerment** (control/optionality maximization)\n",
        "- **Episodic Rehearsal** (long-term skill maintenance)\n",
        "\n",
        "It uses:\n",
        "- Slot attention for object-centric representations\n",
        "- World model ensemble for predictions\n",
        "- Global workspace theory for cognitive routing\n",
        "- Temporal self-awareness for adaptive exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/YOUR_USERNAME/DTC-agent-tpu.git\n",
        "%cd DTC-agent-tpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "# Install TPU dependencies and DTC agent\n",
        "import sys\n",
        "!pip install -q torch torch-xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "!pip install -q -r requirements-colab.txt\n",
        "!pip install -q -e .\n",
        "\n",
        "print(\"\\n✓ Installation complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "verify_tpu"
      },
      "source": [
        "# Verify TPU is available\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "device = xm.xla_device()\n",
        "print(f\"TPU device: {device}\")\n",
        "\n",
        "# Test basic TPU operation\n",
        "x = torch.randn(3, 3, device=device)\n",
        "y = x + 2\n",
        "xm.mark_step()  # Synchronize\n",
        "\n",
        "print(f\"\\n✓ TPU is working correctly!\")\n",
        "print(f\"Number of TPU cores: {xm.xrt_world_size()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "The TPU configuration is optimized for Cloud TPU v2/v3 with 8 cores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "view_config"
      },
      "source": [
        "# View the TPU configuration\n",
        "!cat configs/tpu.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wandb"
      },
      "source": [
        "## 3. Weights & Biases Setup (Optional)\n",
        "\n",
        "W&B tracks training metrics and visualizations. If you don't have an account, skip this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_wandb"
      },
      "source": [
        "# Optional: Login to Weights & Biases\n",
        "import wandb\n",
        "\n",
        "# Uncomment and run to login:\n",
        "# wandb.login()\n",
        "\n",
        "# Or set your API key directly:\n",
        "# import os\n",
        "# os.environ['WANDB_API_KEY'] = 'your-api-key-here'\n",
        "\n",
        "print(\"To use W&B, uncomment the code above and add your API key.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "This will start training the DTC agent on TPU using the Crafter environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train"
      },
      "source": [
        "# Start training on TPU\n",
        "!python -m dtc_agent.training \\\n",
        "    --config configs/tpu.yaml \\\n",
        "    --max-episodes 100 \\\n",
        "    --log-to-wandb false\n",
        "\n",
        "# Notes:\n",
        "# - Set --log-to-wandb true if you configured W&B above\n",
        "# - Adjust --max-episodes based on how long you want to train\n",
        "# - Training will automatically use TPU when available\n",
        "# - Checkpoints are saved in /content/DTC-agent-tpu/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monitor"
      },
      "source": [
        "## 5. Monitor Training\n",
        "\n",
        "View training progress and metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "view_logs"
      },
      "source": [
        "# View recent training logs\n",
        "from dtc_agent.utils import flush_xla_logs\n",
        "\n",
        "print(\"Flushing XLA logs...\")\n",
        "flush_xla_logs()\n",
        "\n",
        "# Or view saved metrics if using W&B:\n",
        "# !wandb login\n",
        "# Then open your W&B dashboard to see live metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checkpoints"
      },
      "source": [
        "## 6. Save and Download Checkpoints\n",
        "\n",
        "Download trained models to continue training locally or for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "list_checkpoints"
      },
      "source": [
        "# List available checkpoints\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "checkpoint_dir = Path(\"/content/DTC-agent-tpu\")\n",
        "checkpoints = list(checkpoint_dir.glob(\"*.pt\"))\n",
        "\n",
        "print(\"Available checkpoints:\")\n",
        "for ckpt in checkpoints:\n",
        "    size_mb = ckpt.stat().st_size / 1024 / 1024\n",
        "    print(f\"  {ckpt.name} ({size_mb:.2f} MB)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_checkpoint"
      },
      "source": [
        "# Download a specific checkpoint\n",
        "from google.colab import files\n",
        "\n",
        "# Replace with your checkpoint filename\n",
        "checkpoint_name = \"dtc_agent_checkpoint_step_1000.pt\"\n",
        "\n",
        "if Path(checkpoint_name).exists():\n",
        "    files.download(checkpoint_name)\n",
        "    print(f\"Downloaded {checkpoint_name}\")\n",
        "else:\n",
        "    print(f\"Checkpoint {checkpoint_name} not found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tips"
      },
      "source": [
        "## Tips and Troubleshooting\n",
        "\n",
        "### Performance Tips\n",
        "- **Batch size**: TPU config uses batch_size=128 per core (effective 1024 total)\n",
        "- **Memory**: If OOM, reduce `episodic_memory.capacity` in config\n",
        "- **Speed**: TPUs are fastest with large batch sizes and minimal host-device transfers\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "1. **TPU not available**\n",
        "   - Check Runtime -> Change runtime type -> Hardware accelerator = TPU\n",
        "   - Restart runtime if needed\n",
        "\n",
        "2. **Out of memory**\n",
        "   - Reduce batch_size in configs/tpu.yaml\n",
        "   - Reduce episodic_memory.capacity\n",
        "   - Reduce world_model_ensemble size\n",
        "\n",
        "3. **Slow training**\n",
        "   - Make sure compile_modules is false (XLA handles compilation)\n",
        "   - Avoid print statements in training loop (use xla_print)\n",
        "   - Check that threading is disabled on TPU\n",
        "\n",
        "4. **Graph breaks**\n",
        "   - XLA requires static graphs\n",
        "   - Dynamic shapes and print statements cause recompilation\n",
        "   - Our code automatically handles this\n",
        "\n",
        "### Resources\n",
        "- [DTC Documentation](https://github.com/YOUR_USERNAME/DTC-agent-tpu)\n",
        "- [PyTorch XLA](https://pytorch.org/xla/release/2.0/index.html)\n",
        "- [Google Cloud TPU](https://cloud.google.com/tpu/docs)\n",
        "\n",
        "### Support\n",
        "If you encounter issues, please open an issue on GitHub with:\n",
        "- Error message\n",
        "- TPU type (v2/v3)\n",
        "- Configuration used\n",
        "- Steps to reproduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup"
      },
      "source": [
        "## 7. Cleanup\n",
        "\n",
        "Free up resources when done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cleanup_code"
      },
      "source": [
        "# Optional: Clear GPU/TPU memory\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(\"✓ Cleanup complete\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
