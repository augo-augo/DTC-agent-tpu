from __future__ import annotations

from typing import Callable, Iterable, Sequence

from typing import Iterable, Sequence

import torch


def decoder_variance_novelty(
    distributions: list[torch.distributions.Distribution],
) -> torch.Tensor:
    """Measure ensemble disagreement via between-model variance.

    Args:
        distributions: List of decoded observation distributions produced by the
            ensemble.

    Returns:
        Tensor capturing novelty derived from mean and variance disagreement.

    Raises:
        ValueError: If ``distributions`` is empty or lacks variance estimates.
    """

    if not distributions:
        raise ValueError("At least one distribution is required")

    means = torch.stack([dist.mean for dist in distributions])

    has_between_variance = len(distributions) >= 2
    if has_between_variance:
        # Use population variance (divide by N) to avoid division by zero when N=1
        # and generally be consistent as unbiased estimator isn't needed here.
        mean_variance = means.var(dim=0, unbiased=False)
    else:
        # If only one model, variance *between* models is zero.
        mean_variance = torch.zeros_like(means[0])

    spatial_dims = tuple(range(1, mean_variance.ndim))
    if spatial_dims:
        mean_variance = mean_variance.mean(dim=spatial_dims)

    if has_between_variance:
        has_between_variance = bool(torch.any(mean_variance > 1e-12))

    variances = []
    for dist in distributions:
        var = getattr(dist, "variance", None)
        if var is None and hasattr(dist, "stddev"):
            var = dist.stddev.pow(2)
        elif var is None:
            raise ValueError("Distribution must expose variance or stddev")

        if not torch.is_tensor(var):
            var = torch.as_tensor(var, device=dist.mean.device)

        if var.ndim > 1:
            var = var.mean(dim=tuple(range(1, var.ndim)))

        variances.append(var)

    avg_variance = torch.stack(variances).mean(dim=0)

    if not has_between_variance:
        novelty = avg_variance.clamp_min(0.0)
        return torch.clamp(novelty, min=1e-6, max=100.0)

    from dtc_agent.utils import safe_log

    log_mean_var = safe_log(mean_variance + 1e-6)
    log_avg_var = safe_log(avg_variance + 1e-6)

    novelty = 3.0 * log_mean_var + log_avg_var
    novelty = novelty * 0.5 + 1.0

    return torch.clamp(novelty, min=0.01, max=100.0)


def ensemble_epistemic_novelty(
    predicted_latents: Sequence[torch.Tensor],
    decoded_distributions: Sequence[torch.distributions.Distribution] | None = None,
    novelty_mix: Sequence[float] | None = None,
) -> torch.Tensor:
    """Combine latent and decoded disagreement into a unified novelty signal.

    Args:
        predicted_latents: Outputs from each dynamics model in the ensemble.
        decoded_distributions: Optional decoded observation distributions
            produced from ``predicted_latents``. When provided the mean
            disagreement between decoded predictions is incorporated into the
            final metric.
        novelty_mix: Optional pair of weights blending latent and decoded
            disagreement contributions.

    Returns:
        Per-batch tensor capturing epistemic uncertainty. Values are lower
        bounded to avoid degenerate zero-novelty regimes when the ensemble fully
        agrees.

    Raises:
        ValueError: If ``predicted_latents`` is empty or not iterable.
    """

    if not isinstance(predicted_latents, Iterable) or len(predicted_latents) == 0:
        raise ValueError("predicted_latents must contain at least one tensor")

    first_latent = predicted_latents[0]
    device = first_latent.device

    if len(predicted_latents) == 1:
        latent_disagreement = torch.zeros(first_latent.shape[0], device=device)
    else:
        stacked = torch.stack(predicted_latents, dim=0)
        latent_std = stacked.std(dim=0, unbiased=False)
        latent_disagreement = latent_std.mean(dim=-1)

    decoded_disagreement = None
    if decoded_distributions and len(decoded_distributions) > 0:
        decoded_means = torch.stack([dist.mean for dist in decoded_distributions])
        decoded_means = decoded_means.float()
        decoded_std = decoded_means.std(dim=0, unbiased=False)
        if decoded_std.ndim > 1:
            decoded_std = decoded_std.mean(dim=tuple(range(1, decoded_std.ndim)))
        decoded_disagreement = decoded_std.to(device=device)

    if decoded_disagreement is None:
        combined = latent_disagreement
    else:
        if novelty_mix is None:
            mix_latent, mix_decoded = 0.5, 0.5
        else:
            mix = tuple(novelty_mix)
            if len(mix) < 2:
                mix_latent, mix_decoded = 0.5, 0.5
            else:
                mix_latent = float(mix[0])
                mix_decoded = float(mix[1])
        mix_latent = max(0.0, mix_latent)
        mix_decoded = max(0.0, mix_decoded)
        total = mix_latent + mix_decoded
        if total <= 0.0:
            mix_latent = mix_decoded = 0.5
            total = 1.0
        mix_latent /= total
        mix_decoded /= total
        combined = mix_latent * latent_disagreement + mix_decoded * decoded_disagreement

    return combined.clamp_min(0.01)


class AdaptiveNoveltyCalculator:
    """Dynamically scales novelty estimates to maintain a target mean."""

    def __init__(
        self,
        target_mean: float = 1.0,
        momentum: float = 0.1,
        base_metric: Callable[[list[torch.distributions.Distribution]], torch.Tensor]
        | None = None,
    ) -> None:
        self.target_mean = target_mean
        self.momentum = momentum
        self.base_metric = base_metric or decoder_variance_novelty
        self.running_mean: float | None = None
        self.scale = 1.0

    def __call__(
        self, inputs
    ) -> torch.Tensor:
        """Scale raw novelty to maintain the configured running mean.

        Args:
            inputs: Either a tensor of novelty values or inputs consumed by
                ``base_metric`` to derive novelty.

        Returns:
            Tensor of scaled novelty values bounded within reasonable limits.

        Raises:
            ValueError: If a non-tensor input is supplied without ``base_metric``.
        """
        if isinstance(inputs, torch.Tensor):
            raw_novelty = inputs
        else:
            if self.base_metric is None:
                raise ValueError("A base_metric must be provided for non-tensor inputs")
            raw_novelty = self.base_metric(inputs)

        current_mean = raw_novelty.mean().item()
        if self.running_mean is None:
            self.running_mean = current_mean
        else:
            self.running_mean = (
                (1 - self.momentum) * self.running_mean
                + self.momentum * current_mean
            )

        if self.running_mean > 0:
            self.scale = self.target_mean / self.running_mean

        scaled_novelty = raw_novelty * self.scale
        return torch.clamp(scaled_novelty, min=0.01, max=100.0)


