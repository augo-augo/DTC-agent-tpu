# TPU-Optimized Configuration for Google Colab
# Extends default.yaml with XLA-specific settings
#
# Tested on: Google Colab TPU v2/v3 (8 cores)
# Total HBM: ~128GB (16GB per core)

defaults:
  - default

device: xla

# Batch size tuning for TPU
# TPU v2/v3 has 8 cores with 16GB HBM each
# Larger batches = better TPU utilization
batch_size: 128  # Per core batch size (effective total = 128 Ã— 8 = 1024)
rollout_capacity: 8192  # Larger rollout buffer for big batches

# XLA/TPU specific
compile_modules: false  # Disable torch.compile (XLA handles compilation)

# World Model - balanced for memory/performance
world_model_ensemble: 5  # Good ensemble diversity without OOM
encoder:
  num_slots: 8  # Object-centric slots
  slot_dim: 128
  hidden_dim: 256
  num_iterations: 3  # Slot attention iterations

decoder:
  hidden_dim: 256
  num_layers: 3
  init_log_std: -1.0

dynamics:
  hidden_dim: 512
  num_layers: 2
  action_dim: 18  # Crafter has 18 actions

# Episodic Memory - TPU-compatible (no FAISS)
# Uses pure PyTorch k-NN on TPU
episodic_memory:
  capacity: 8000  # Reduced slightly for Colab free tier
  key_dim: 128

# Actor-Critic
actor:
  hidden_dim: 512
  num_layers: 3
  dropout: 0.1

critic:
  hidden_dim: 512
  num_layers: 3
  dropout: 0.1

# Empowerment
empowerment:
  hidden_dim: 256
  projection_dim: 64
  temperature: 0.1
  queue_size: 4096

# Optimization
optimizer_lr: 0.0003  # Lower LR for large batch size
gradient_clip_norm: 1.0

# Cognitive Workspace
workspace:
  broadcast_slots: 4
  attention_temperature: 0.5
  progress_momentum: 0.95
  action_cost_scale: 0.01
  self_bias: 0.1

# Temporal Self
temporal_self:
  field_dim: 32
  ema_fast: 0.3
  ema_slow: 0.01
  actor_entropy_scale: 0.5

# Intrinsic Reward
reward:
  competence_weight: 1.0
  empowerment_weight: 0.3
  survival_weight: 0.1
  explore_weight: 0.5

# Dreaming
max_dream_horizon: 30  # Reduced for faster iteration
dream_every_n_steps: 4
max_horizon_multiplier: 2.0

# Checkpointing
policy_snapshot_interval: 100
frozen_decoder_refresh_interval: 1000

# Environment (Colab headless mode)
self_state_dim: 0  # No proprioception in Crafter

# Logging
# Set log_to_wandb: true if you want W&B integration
log_to_wandb: false
